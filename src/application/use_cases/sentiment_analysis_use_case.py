# src/application/use_cases/sentiment_analysis_use_case.py

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import traceback

import json
from collections import Counter

# Custom Module Imports
from application.agents.naver_review.naver_review_agent import NaverReviewAgent
from src.infrastructure.external_services.naver_search.naver_review_api import (
    search_naver_blog,
)
from src.application.core.graph import app_llm_graph
from src.application.core.utils import (
    save_df_to_csv,
    summarize_negative_feedback,
)
from src.infrastructure.reporting.charts import (
    create_donut_chart,
    create_stacked_bar_chart,
    create_satisfaction_level_bar_chart,
    create_absolute_score_line_chart,
    create_outlier_boxplot,
)
from src.infrastructure.reporting.wordclouds import create_sentiment_wordclouds
from src.application.core.constants import CATEGORY_TO_ICON_MAP
from src.infrastructure.llm_client import get_llm_client
from src.domain.knowledge_base import knowledge_base


class SentimentAnalysisUseCase:
    def __init__(self, naver_supervisor: NaverReviewAgent, script_dir: str):
        self.naver_supervisor = naver_supervisor
        self.script_dir = script_dir
        self.llm = get_llm_client(temperature=0.1)

    async def _generate_distribution_interpretation(self, counts: dict, total_sentences: int, boundaries: dict, avg_score: float) -> str:
        if total_sentences == 0:
            return "ë¶„ì„í•  ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤."

        prompt = f"""
        You are an expert data analyst specializing in customer feedback analysis. Your task is to write a comprehensive, easy-to-understand analysis of a festival's sentiment distribution in Korean, based on the data provided. The analysis must be objective, data-driven, and strictly follow the requested markdown format.

        **Input Data:**
        - Total Sentences Analyzed: {total_sentences}
        - Sentence Counts per Satisfaction Level: {counts}
        - Absolute Average Score (from -5.0 to 5.0): {avg_score:.2f}
        - Satisfaction Level Boundaries:
            - Very Dissatisfied: < {boundaries['very_dissatisfied_upper']:.2f}
            - Dissatisfied: < {boundaries['dissatisfied_upper']:.2f}
            - Neutral: < {boundaries['neutral_upper']:.2f}
            - Satisfied: < {boundaries['satisfied_upper']:.2f}
            - Very Satisfied: >= {boundaries['satisfied_upper']:.2f}

        **Instructions:**
        1.  **Role:** Act as a professional data analyst.
        2.  **Tone:** Write in clear, objective, and helpful Korean.
        3.  **Format:** You MUST follow this markdown format exactly. Do not add any other sections.

            ```markdown
            ### ğŸ“Š ë§Œì¡±ë„ ë¶„í¬ ì¢…í•© ë¶„ì„

            **ì£¼ìš” ì§€í‘œ:**
            - **ë¶„ì„ ë¬¸ì¥ ìˆ˜**: {total_sentences}ê°œ
            - **í‰ê·  ë§Œì¡±ë„ ì ìˆ˜**: {avg_score:.2f}ì  (5ì  ë§Œì )
            - **ë§Œì¡±ë„ Level ê¸°ì¤€**: (ë§¤ìš° ë¶ˆë§Œì¡± < {boundaries['very_dissatisfied_upper']:.2f} < ë¶ˆë§Œì¡± < {boundaries['dissatisfied_upper']:.2f} < ë³´í†µ < {boundaries['neutral_upper']:.2f} < ë§Œì¡± < {boundaries['satisfied_upper']:.2f} < ë§¤ìš° ë§Œì¡±)

            **ë¶„í¬ í˜•íƒœ ë¶„ì„:**
            [Based on the sentence counts, describe the shape of the distribution. Use one of the following patterns:
            - **ì••ë„ì  ê¸ì • (J-ì»¤ë¸Œí˜•):** If 'ë§¤ìš° ë§Œì¡±' and 'ë§Œì¡±' counts are overwhelmingly dominant.
            - **ì–‘ê·¹í™” (U-ì»¤ë¸Œí˜•):** If 'ë§¤ìš° ë§Œì¡±' and 'ë§¤ìš° ë¶ˆë§Œì¡±' have the highest counts, and 'ë³´í†µ' has a low count.
            - **ë‹¤ì–‘í•œ í‰ê°€ (í‰í‰í•œ ë¶„í¬):** If counts are spread out across all levels without a clear single peak.
            - **ë³´í†µ ì¤‘ì‹¬ (ì¢…í˜• ë¶„í¬):** If 'ë³´í†µ' has the highest count, with other levels tapering off.
            - **ë¶€ì •ì  í‰ê°€ (L-ì»¤ë¸Œí˜•):** If 'ë§¤ìš° ë¶ˆë§Œì¡±' and 'ë¶ˆë§Œì¡±' counts are overwhelmingly dominant.
            Provide a brief, one-sentence description of the shape in Korean.]

            **ì¢…í•© í•´ì„:**
            [Synthesize all the information into a final conclusion in Korean. CRITICALLY, interpret the distribution shape in the context of the **Absolute Average Score**.
            - If the average score is very high (e.g., > 2.5), explain that even the 'ë³´í†µ(Neutral)' category likely represents positive opinions, making the overall feedback very strong.
            - If the average score is very low (e.g., < 0.0), explain that even 'ë³´í†µ(Neutral)' might indicate dissatisfaction.
            - If the distribution is polarized, recommend looking into the specific causes.
            - If the distribution is diverse/flat, mention that visitors had varied experiences with different aspects of the event.
            Provide a 2-3 sentence summary in Korean.]
            ```
        4.  **Constraint:** Generate only the markdown content as requested. Do not add any introductory or concluding remarks outside of the specified format.
        """
        try:
            response = await self.llm.ainvoke(prompt)
            # Extract markdown content if the LLM wraps it in ```markdown
            match = re.search(r"```markdown\n(.*)```", response.content, re.DOTALL)
            if match:
                return match.group(1).strip()
            return response.content.strip()
        except Exception as e:
            print(f"Error generating LLM-based distribution interpretation: {e}")
            return "ìë™ ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _format_positive_keywords_html(
        self, keywords_data: list, total_reviews: int
    ) -> str:
        if not keywords_data:
            return ""

        # Sort by count descending
        keywords_data.sort(key=lambda x: x.get("count", 0), reverse=True)

        max_count = keywords_data[0].get("count", 0) if keywords_data else 0

        html = '<div style="padding: 10px; border: 1px solid #e0e0e0; border-radius: 8px;">'
        html += f'<h3 style="margin-bottom: 15px; font-size: 1.1em;">ğŸ‘ ì´ëŸ° ì ì´ ì¢‹ì•˜ì–´ìš” <span style="font-size: 0.8em; color: #777;">(ì´ {total_reviews}ê°œ í›„ê¸° ê¸°ë°˜)</span></h3>'
        html += '<ul style="list-style-type: none; padding: 0; margin: 0;">'

        # Define some nice icons (using unicode)
        icons = ["ğŸ˜‹", "âœ¨", "ğŸ’–", "ğŸ‘", "ğŸ‰", "ğŸ’¯", "â­", "ğŸ’¡", "ğŸ™Œ", "ğŸ˜"]

        for i, item in enumerate(keywords_data[:10]):  # Show top 10
            keyword = item.get("keyword", "N/A")
            count = item.get("count", 0)
            width_percentage = (count / max_count) * 100 if max_count > 0 else 0
            icon = icons[i % len(icons)]

            html += f"""
            <li style="margin-bottom: 8px; position: relative; background-color: #f7f7f7; border-radius: 4px; overflow: hidden;">
                <div style="position: absolute; top: 0; left: 0; height: 100%; width: {width_percentage}%; background-color: #D6E6FF; z-index: 1;"></div>
                <div style="position: relative; z-index: 2; padding: 8px 12px; display: flex; align-items: center; justify-content: space-between;">
                    <span style="font-size: 0.95em; color: #333;">{icon} "{keyword}"</span>
                    <span style="font-size: 0.9em; font-weight: bold; color: #005AAB;">{count}</span>
                </div>
            </li>
            """
        html += "</ul></div>"
        return html

    async def _generate_positive_keywords_summary(
        self, aspect_sentiment_pairs: list
    ) -> list:
        if not aspect_sentiment_pairs:
            return []

        # Filter for positive sentiment pairs
        positive_pairs = []
        sentiment_dictionaries = {
            **knowledge_base.adjectives,
            **knowledge_base.adverbs,
            **knowledge_base.sentiment_nouns,
            **knowledge_base.idioms,
        }
        for aspect, sentiment in aspect_sentiment_pairs:
            if sentiment in sentiment_dictionaries:
                scores = sentiment_dictionaries[sentiment]
                if scores and any(s > 0 for s in scores):
                    positive_pairs.append((aspect, sentiment))

        if not positive_pairs:
            return []

        # Use Counter to get initial frequencies
        pair_counts = Counter(positive_pairs)
        # Convert to a list of strings for the LLM prompt
        pairs_str_list = [
            f"('{p[0]}', '{p[1]}'): {c}íšŒ" for p, c in pair_counts.items()
        ]

        prompt = f"""
        ë‹¹ì‹ ì€ ì‚¬ìš©ì ë¦¬ë·°ì—ì„œ í•µì‹¬ ê¸ì • í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ê·¸ë£¹í™”í•˜ëŠ” ë§ˆì¼€íŒ… ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ëŠ” 'ì£¼ì²´-ê°ì„±' ìŒê³¼ ê° ìŒì˜ ì–¸ê¸‰ íšŸìˆ˜ ëª©ë¡ì…ë‹ˆë‹¤.

        [ë°ì´í„°]
        {', '.join(pairs_str_list)}

        [ìš”ì²­]
        1. ì˜ë¯¸ê°€ ìœ ì‚¬í•œ 'ì£¼ì²´-ê°ì„±' ìŒë“¤ì„ í•˜ë‚˜ì˜ ëŒ€í‘œ í‚¤ì›Œë“œë¡œ ê·¸ë£¹í™”í•´ì£¼ì„¸ìš”.
           (ì˜ˆ: ('ìŒì‹', 'ë§›ìˆë‹¤'), ('ìŒì‹', 'í›Œë¥­í•˜ë‹¤') -> "ìŒì‹ì´ ë§›ìˆì–´ìš”")
           (ì˜ˆ: ('ì§ì›', 'ì¹œì ˆí•˜ë‹¤'), ('ì‚¬ì¥ë‹˜', 'ì¹œì ˆí•˜ë‹¤') -> "ì§ì›ì´ ì¹œì ˆí•´ìš”")
        2. ê° ëŒ€í‘œ í‚¤ì›Œë“œì— ëª‡ ê°œì˜ ì›ë³¸ ìŒì´ í¬í•¨ë˜ì—ˆëŠ”ì§€ í•©ì‚°í•˜ì—¬ `count`ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”.
        3. ìµœì¢… ê²°ê³¼ëŠ” ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ í˜•íƒœì˜ `keyword`ì™€ `count`ë¥¼ í¬í•¨í•˜ëŠ” JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        4. ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ìˆœì„œë¡œ ì •ë ¬í•´ì£¼ì„¸ìš”.
        5. ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSON ë¦¬ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.

        [ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
        [
            {{"keyword": "ìŒì‹ì´ ë§›ìˆì–´ìš”", "count": 21}},
            {{"keyword": "ì¬ë£Œê°€ ì‹ ì„ í•´ìš”", "count": 6}},
            {{"keyword": "ë§¤ì¥ì´ ë„“ì–´ìš”", "count": 6}},
            {{"keyword": "ì§ì›ì´ ì¹œì ˆí•´ìš”", "count": 5}}
        ]
        """
        try:
            response = await self.llm.ainvoke(prompt)
            # Extract JSON from the response
            json_str_match = re.search(r"\[.*\]", response.content, re.DOTALL)
            if json_str_match:
                return json.loads(json_str_match.group())
            return []
        except (json.JSONDecodeError, Exception) as e:
            print(f"Error generating positive keywords summary: {e}")
            return []

    def _calculate_satisfaction_boundaries(self, scores: list) -> dict:
        if not scores:
            return {
                "boundaries": {},
                "filtered_scores": [],
                "outliers": [],
            }

        q1 = np.percentile(scores, 25)
        q3 = np.percentile(scores, 75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr

        filtered_scores = [s for s in scores if lower_bound <= s <= upper_bound]
        outliers = [s for s in scores if s < lower_bound or s > upper_bound]

        if not filtered_scores:
            filtered_scores = scores

        mean = np.mean(filtered_scores)
        std = np.std(filtered_scores)

        # Handle case where all scores are identical
        if np.isclose(std, 0):
            std = 0.1

        boundaries = {
            "mean": mean,
            "std": std,
            "very_dissatisfied_upper": mean - 1.5 * std,
            "dissatisfied_upper": mean - 0.5 * std,
            "neutral_upper": mean + 0.5 * std,
            "satisfied_upper": mean + 1.5 * std,
        }
        return {
            "boundaries": boundaries,
            "filtered_scores": filtered_scores,
            "outliers": outliers,
        }

    def _map_score_to_level(self, score: float, boundaries: dict) -> int:
        if not boundaries:
            return 3  # Default to neutral if no boundaries
        if score < boundaries["very_dissatisfied_upper"]:
            return 1  # ë§¤ìš° ë¶ˆë§Œì¡±
        elif score < boundaries["dissatisfied_upper"]:
            return 2  # ë¶ˆë§Œì¡±
        elif score < boundaries["neutral_upper"]:
            return 3  # ë³´í†µ
        elif score < boundaries["satisfied_upper"]:
            return 4  # ë§Œì¡±
        else:
            return 5  # ë§¤ìš° ë§Œì¡±

    def _remove_leading_year(self, festival_name: str) -> str:
        # Regex to find a four-digit year at the beginning of the string, optionally followed by 'ë…„' and a space
        # e.g., "2025 êµ­ë¯¼ê³ í–¥ ë‚¨í•´ ë§ˆì‹œê³  RUN í›„ê¸°" -> "êµ­ë¯¼ê³ í–¥ ë‚¨í•´ ë§ˆì‹œê³  RUN í›„ê¸°"
        # e.g., "2024ë…„ ì„œìš¸ ë¶ˆê½ƒì¶•ì œ" -> "ì„œìš¸ ë¶ˆê½ƒì¶•ì œ"
        # e.g., "2023-2024 ê²¨ìš¸ ì¶•ì œ" -> "2024 ê²¨ìš¸ ì¶•ì œ" (only removes the first year if followed by space)
        cleaned_name = re.sub(r"^\d{4}\s*ë…„?\s*", "", festival_name).strip()
        return cleaned_name if cleaned_name else festival_name # Return original if only year was present or no change

    async def analyze_sentiment(self, festival_name: str, num_reviews: int):
        if not festival_name:
            raise ValueError("ì¶•ì œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")

        # Preprocess festival_name to remove leading year
        processed_festival_name = self._remove_leading_year(festival_name)
        print(f"Original festival name: {festival_name}, Processed: {processed_festival_name}")

        search_keyword = f"{processed_festival_name} í›„ê¸°"

        blog_results_list = []
        blog_judgments_list = []
        all_scores = []
        all_negative_sentences = []
        all_aspect_sentiment_pairs = []
        total_pos, total_neg = 0, 0

        start_index = 1
        max_results_to_scan = 100
        display_count = 20
        consecutive_skips = 0

        while (
            len(blog_results_list) < num_reviews and start_index < max_results_to_scan
        ):
            api_results = search_naver_blog(
                search_keyword, display=display_count, start=start_index
            )
            if not api_results:
                break

            candidate_blogs = [
                item for item in api_results if "blog.naver.com" in item["link"]
            ]
            for blog_data in candidate_blogs:
                if len(blog_results_list) >= num_reviews:
                    break
                try:
                    content, _ = await self.naver_supervisor._scrape_blog_content(
                        blog_data["link"]
                    )
                    if (
                        not content
                        or "ì˜¤ë¥˜" in content
                        or "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in content
                    ):
                        consecutive_skips += 1
                        continue

                    content = content[:30000]

                    final_state = app_llm_graph.invoke(
                        {
                            "original_text": content,
                            "keyword": processed_festival_name,
                            "title": re.sub(r"<[^>]+>", "", blog_data["title"]).strip(),
                            "log_details": True,
                        }
                    )

                    if (
                        not final_state
                        or not final_state.get("is_relevant")
                        or not final_state.get("final_judgments")
                    ):
                        consecutive_skips += 1
                        continue

                    consecutive_skips = 0
                    judgments = final_state.get("final_judgments", [])
                    aspect_pairs = final_state.get("aspect_sentiment_pairs", [])
                    blog_judgments_list.append(judgments)
                    all_scores.extend([j["score"] for j in judgments])
                    all_aspect_sentiment_pairs.extend(aspect_pairs)

                    blog_results_list.append(
                        {
                            "ë¸”ë¡œê·¸ ì œëª©": re.sub(
                                r"<[^>]+>", "", blog_data["title"]
                            ).strip(),
                            "ë§í¬": blog_data["link"],
                            "postdate": blog_data.get("postdate", ""),
                            "judgments": judgments,
                        }
                    )

                except Exception as e:
                    print(
                        f"ë¸”ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ({festival_name}, {blog_data.get('link', 'N/A')}): {e}"
                    )
                    traceback.print_exc()
                    consecutive_skips += 1
                    continue

                if consecutive_skips >= 3:
                    break
            if len(blog_results_list) >= num_reviews or consecutive_skips >= 3:
                break
            start_index += display_count

        if not blog_results_list:
            raise ValueError(
                f"'{festival_name}'ì— ëŒ€í•œ ìœ íš¨í•œ í›„ê¸° ë¸”ë¡œê·¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            )

        boundary_results = self._calculate_satisfaction_boundaries(all_scores)
        boundaries = boundary_results["boundaries"]
        outliers = boundary_results["outliers"]

        processed_blog_results = []
        all_satisfaction_levels = []

        for blog in blog_results_list:
            judgments = blog["judgments"]
            blog_satisfaction_levels = []

            pos_count = 0
            neg_count = 0

            for j in judgments:
                level = self._map_score_to_level(j["score"], boundaries)
                j["satisfaction_level"] = level
                blog_satisfaction_levels.append(level)
                all_satisfaction_levels.append(level)

                if j["final_verdict"] == "ê¸ì •":
                    pos_count += 1
                else:
                    neg_count += 1
                    all_negative_sentences.append(j["sentence"])

            total_pos += pos_count
            total_neg += neg_count

            avg_satisfaction = (
                np.mean(blog_satisfaction_levels) if blog_satisfaction_levels else 3.0
            )
            pos_perc = (
                (pos_count / (pos_count + neg_count) * 100)
                if (pos_count + neg_count) > 0
                else 0
            )
            neg_perc = (
                (neg_count / (pos_count + neg_count) * 100)
                if (pos_count + neg_count) > 0
                else 0
            )

            processed_blog_results.append(
                {
                    "ë¸”ë¡œê·¸ ì œëª©": blog["ë¸”ë¡œê·¸ ì œëª©"],
                    "ë§í¬": blog["ë§í¬"],
                    "ê°ì„± ë¹ˆë„": len(judgments),
                    "ê°ì„± ì ìˆ˜": f"{avg_satisfaction:.2f} / 5",
                    "ê¸ì • ë¬¸ì¥ ìˆ˜": pos_count,
                    "ë¶€ì • ë¬¸ì¥ ìˆ˜": neg_count,
                    "ê¸ì • ë¹„ìœ¨ (%)": f"{pos_perc:.1f}",
                    "ë¶€ì • ë¹„ìœ¨ (%)": f"{neg_perc:.1f}",
                    "ê¸/ë¶€ì • ë¬¸ì¥ ìš”ì•½": "<br>---<br>".join(
                        [
                            f"[{j['final_verdict']}({j['satisfaction_level']}ì )] {j['sentence']}"
                            for j in judgments
                        ]
                    ),
                    "ë§Œì¡±ë„ ì ìˆ˜": f"{avg_satisfaction:.2f} / 5",  # Keep for backwards compatibility
                }
            )

        overall_avg_satisfaction = (
            np.mean(all_satisfaction_levels) if all_satisfaction_levels else 3.0
        )
        
        # --- New: Satisfaction Level Counting and Interpretation ---
        level_map = {1: "ë§¤ìš° ë¶ˆë§Œì¡±", 2: "ë¶ˆë§Œì¡±", 3: "ë³´í†µ", 4: "ë§Œì¡±", 5: "ë§¤ìš° ë§Œì¡±"}
        satisfaction_counts = Counter(
            [level_map.get(level, "ë³´í†µ") for level in all_satisfaction_levels]
        )
        
        distribution_chart_fig = None
        if all_satisfaction_levels:
            fig = create_satisfaction_level_bar_chart(
                satisfaction_counts, f"{festival_name} ìƒëŒ€ì  ë§Œì¡±ë„ ë¶„í¬"
            )
            if fig:
                distribution_chart_fig = fig
                # Save for debugging/legacy use if needed, but don't close
                distribution_chart_path = os.path.join(
                    self.script_dir, "temp_img", f"dist_chart_{festival_name}.png"
                )
                os.makedirs(os.path.dirname(distribution_chart_path), exist_ok=True)
                fig.savefig(distribution_chart_path)
                # plt.close(fig) # Removed to pass fig object

        absolute_chart_fig = None
        if all_scores:
            fig = create_absolute_score_line_chart(
                all_scores, f"{festival_name} ì ˆëŒ€ ì ìˆ˜ ë¶„í¬"
            )
            if fig:
                absolute_chart_fig = fig
                absolute_chart_path = os.path.join(
                    self.script_dir, "temp_img", f"abs_chart_{festival_name}.png"
                )
                os.makedirs(os.path.dirname(absolute_chart_path), exist_ok=True)
                fig.savefig(absolute_chart_path)
                # plt.close(fig) # Removed to pass fig object

        distribution_description = await self._generate_distribution_interpretation(
            satisfaction_counts, len(all_satisfaction_levels), boundaries, overall_avg_satisfaction
        )
        # --- End New ---

        outlier_chart_fig = None
        if all_scores:
            fig = create_outlier_boxplot(
                all_scores, f"{festival_name} ê°ì„± ì ìˆ˜ ì´ìƒì¹˜"
            )
            if fig:
                outlier_chart_fig = fig
                outlier_chart_path = os.path.join(
                    self.script_dir, "temp_img", f"outlier_chart_{festival_name}.png"
                )
                os.makedirs(os.path.dirname(outlier_chart_path), exist_ok=True)
                fig.savefig(outlier_chart_path)
                # plt.close(fig) # Removed to pass fig object

        neg_summary_text = summarize_negative_feedback(all_negative_sentences)
        overall_summary_text = f"- **ì´ ë¶„ì„ ë¸”ë¡œê·¸**: {len(blog_results_list)}ê°œ\n- **ì „ì²´ í‰ê·  ë§Œì¡±ë„**: {overall_avg_satisfaction:.2f} / 5.0 ì \n- **ê¸ì • ë¬¸ì¥ ìˆ˜**: {total_pos}ê°œ\n- **ë¶€ì • ë¬¸ì¥ ìˆ˜**: {total_neg}ê°œ"

        summary_df = pd.DataFrame(
            [
                {
                    "ì¶•ì œëª…": festival_name,
                    "í‰ê·  ë§Œì¡±ë„": f"{overall_avg_satisfaction:.2f}",
                    "ê¸ì • ë¬¸ì¥ ìˆ˜": total_pos,
                    "ë¶€ì • ë¬¸ì¥ ìˆ˜": total_neg,
                }
            ]
        )
        summary_csv_path = save_df_to_csv(summary_df, "overall_summary", festival_name)

        blog_df = pd.DataFrame(processed_blog_results)
        blog_list_csv_path = save_df_to_csv(blog_df, "blog_list", festival_name)

        overall_chart = create_donut_chart(
            total_pos, total_neg, f"{festival_name} ì „ì²´ í›„ê¸° ìš”ì•½"
        )

        # Generate "What I liked" summary
        positive_keywords_data = await self._generate_positive_keywords_summary(
            all_aspect_sentiment_pairs
        )
        positive_keywords_html = self._format_positive_keywords_html(
            positive_keywords_data, len(blog_results_list)
        )

        print(f"DEBUG: all_aspect_sentiment_pairs: {all_aspect_sentiment_pairs}") # DEBUG PRINT

        # Prepare chart data for frontend rendering
        donut_data = {
            "positive": total_pos,
            "negative": total_neg,
        }

        satisfaction_data = {
            "labels": ["ë§¤ìš° ë¶ˆë§Œì¡±", "ë¶ˆë§Œì¡±", "ë³´í†µ", "ë§Œì¡±", "ë§¤ìš° ë§Œì¡±"],
            "counts": [satisfaction_counts.get(label, 0) for label in ["ë§¤ìš° ë¶ˆë§Œì¡±", "ë¶ˆë§Œì¡±", "ë³´í†µ", "ë§Œì¡±", "ë§¤ìš° ë§Œì¡±"]],
        }

        # Prepare absolute score distribution data
        bins = [-np.inf, -2.0, -1.0, 0.0, 1.0, 2.0, np.inf]
        labels_abs = ['ë§¤ìš° ë¶€ì • (<-2)', 'ë¶€ì • (-2~-1)', 'ì•½ê°„ ë¶€ì • (-1~0)', 'ì•½ê°„ ê¸ì • (0~1)', 'ê¸ì • (1~2)', 'ë§¤ìš° ê¸ì • (>2)']
        hist, _ = np.histogram(all_scores, bins=bins) if all_scores else (np.zeros(len(labels_abs)), None)
        absolute_data = {
            "labels": labels_abs,
            "counts": hist.tolist(),
        }

        # Prepare outlier data
        q1 = np.percentile(all_scores, 25) if all_scores else 0
        q3 = np.percentile(all_scores, 75) if all_scores else 0
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        median = np.median(all_scores) if all_scores else 0
        outlier_data = {
            "min": float(np.min(all_scores)) if all_scores else 0,
            "q1": float(q1),
            "median": float(median),
            "q3": float(q3),
            "max": float(np.max(all_scores)) if all_scores else 0,
            "lower_bound": float(lower_bound),
            "upper_bound": float(upper_bound),
            "outliers": [float(s) for s in all_scores if s < lower_bound or s > upper_bound] if all_scores else [],
        }

        return {
            "positive_keywords_html": positive_keywords_html,
            "neg_summary_text": neg_summary_text,
            "overall_summary_text": overall_summary_text,
            "summary_csv_path": summary_csv_path,
            "blog_df": blog_df,
            "blog_judgments_list": blog_judgments_list,
            "blog_list_csv_path": blog_list_csv_path,
            "overall_chart": overall_chart,
            "distribution_chart": distribution_chart_fig,
            "absolute_chart": absolute_chart_fig,
            "distribution_description": distribution_description,
            "outlier_chart": outlier_chart_fig,
            "total_score_count": len(all_scores),
            "outlier_count": len(outliers),
            "all_aspect_sentiment_pairs": all_aspect_sentiment_pairs,
            # Chart data for frontend
            "donut_data": donut_data,
            "satisfaction_data": satisfaction_data,
            "absolute_data": absolute_data,
            "outlier_data": outlier_data,
        }
